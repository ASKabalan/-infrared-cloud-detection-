%% Copernicus Publications Manuscript Preparation Template for LaTeX Submissions
%% ---------------------------------
%% This template should be used for copernicus.cls
%% The class file and some style files are bundled in the Copernicus Latex Package, which can be downloaded from the different journal webpages.
%% For further assistance please contact Copernicus Publications at: production@copernicus.org
%% https://publications.copernicus.org/for_authors/manuscript_preparation.html


%% Please use the following documentclass and journal abbreviations for preprints and final revised papers.

%% 2-column papers and preprints
\documentclass[amt, article]{copernicus}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\usepackage{natbib}
\bibliographystyle{copernicus}
\usepackage{hyperref}    % Create hyperlinks
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!100!black},
    citecolor={blue!100!black},
    urlcolor={blue!100!black}
}
%\usepackage[switch]{lineno}
%\usepackage{siunitx,etoolbox}
%% Journal abbreviations (please use the same for preprints and final revised papers)
\usepackage{svg}
\newcommand{\orcid}[1]{\text{\href{https://orcid.org/#1}{\includegraphics[width=10pt]{orcidlogo.pdf}}}}

% Advances in Geosciences (adgeo)
% Advances in Radio Science (ars)
% Advances in Science and Research (asr)
% Advances in Statistical Climatology, Meteorology and Oceanography (ascmo)
% Aerosol Research (ar)
% Annales Geophysicae (angeo)
% Archives Animal Breeding (aab)
% Atmospheric Chemistry and Physics (acp)
% Atmospheric Measurement Techniques (amt)
% Biogeosciences (bg)
% Climate of the Past (cp)
% DEUQUA Special Publications (deuquasp)
% Earth Surface Dynamics (esurf)
% Earth System Dynamics (esd)
% Earth System Science Data (essd)
% E&G Quaternary Science Journal (egqsj)
% EGUsphere (egusphere) | This is only for EGUsphere preprints submitted without relation to an EGU journal.
% European Journal of Mineralogy (ejm)
% Fossil Record (fr)
% Geochronology (gchron)
% Geographica Helvetica (gh)
% Geoscience Communication (gc)
% Geoscientific Instrumentation, Methods and Data Systems (gi)
% Geoscientific Model Development (gmd)
% History of Geo- and Space Sciences (hgss)
% Hydrology and Earth System Sciences (hess)
% Journal of Bone and Joint Infection (jbji)
% Journal of Micropalaeontology (jm)
% Journal of Sensors and Sensor Systems (jsss)
% Magnetic Resonance (mr)
% Mechanical Sciences (ms)
% Natural Hazards and Earth System Sciences (nhess)
% Nonlinear Processes in Geophysics (npg)
% Ocean Science (os)
% Polarforschung - Journal of the German Society for Polar Research (polf)
% Primate Biology (pb)
% Proceedings of the International Association of Hydrological Sciences (piahs)
% Safety of Nuclear Waste Disposal (sand)
% Scientific Drilling (sd)
% SOIL (soil)
% Solid Earth (se)
% State of the Planet (sp)
% The Cryosphere (tc)
% Weather and Climate Dynamics (wcd)
% Web Ecology (we)
% Wind Energy Science (wes)


%% \usepackage commands included in the copernicus.cls:
%\usepackage[german, english]{babel}
%\usepackage{tabularx}
%\usepackage{cancel}
%\usepackage{multirow}
%\usepackage{supertabular}
%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{amsthm}
%\usepackage{float}
%\usepackage{subfig}
%\usepackage{rotating}


\begin{document}

\title{IRIS-CloudDeep: Infrared Radiometric Image classification and Segmentation of Cloud structure using Deep-learning framework for ground-based long-wave infrared thermal camera observations}


% \Author[affil]{given_name}{surname}

\Author[1,*]{Kélian}{Sommer \orcid{0009-0007-5504-5838} }
\Author[2,*]{Wassim}{Kabalan}
\Author[3,*]{Romain}{Brunet}

\affil[1]{Laboratoire Univers et Particules de Montpellier, Université de Montpellier, CNRS, Montpellier, France}
\affil[2]{Université Paris Cité, CNRS, Astroparticule et Cosmologie, F-75013 Paris, France}
\affil[3]{Laboratoire d’Astrophysique de Marseille, Marseille, France}

%% The [] brackets identify the author with the corresponding affiliation. 1, 2, 3, etc. should be inserted.

%% If an author is deceased, please mark the respective author name(s) with a dagger, e.g. "\Author[2,$\dag$]{Anton}{Smith}", and add a further "\affil[$\dag$]{deceased, 1 July 2019}".

%% If authors contributed equally, please mark the respective author names with an asterisk, e.g. "\Author[2,*]{Anton}{Smith}" and "\Author[3,*]{Bradley}{Miller}" and add a further affiliation: "\affil[*]{These authors contributed equally to this work.}".


\correspondence{Kélian Sommer (kelian.sommer@umontpellier.fr), Wassim Kabalan (wassim@apc.in2p3.fr) and Romain Brunet (romain.brunet@lam.fr)}

\runningtitle{Infrared Radiometric Image classification and Segmentation of Cloud structure}

\runningauthor{K. Sommer et al.}

\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

%% These dates will be inserted by Copernicus Publications during the typesetting process.


\firstpage{1}

\maketitle



\begin{abstract}
    Infrared thermal cameras offer reliable means of assessing atmospheric conditions by measuring the downard radiance from the sky, facilitating their usage in cloud monitoring endeavors. Precise identification and detection of clouds in images poses great challenges stemming from the indistinct boundaries inherent to cloud formations. Various methodologies for segmentation have been previously suggested. Most of them rely on color as the distinguishing criterion for cloud identification in the visible spectral domain and thus lack the ability to detect cloud structure on gray-scaled images with satisfying accuracy. In this work, we propose a new complete deep-learning framework to perform image classification with  Convolutional Neural Networks (CNNs) and image segmentation by exploring the use of U-Net model. We demonstrate the effectiveness of this technique by conducting a series of tests and validations on self-captured infrared sky images and transformed publicly available datasets. Our findings reveal that the models can effectively differentiate between image types and accurately capture detailed cloud structure information, even when trained with a single binary ground-truth mask per input sample. We also compare our framework with other state-of-the-art machine-learning methods and conclude that it outperforms them when applied to infrared data. The classifier model achieves an excellent accuracy of 93\% in image types distinction, while the segmentation model attains a mean pixel accuracy of 94\% on our own dataset. We emphasize that our framework exhibits strong viability and can be used for infrared thermal ground-based cloud monitoring operations over extended durations.
\end{abstract}

%\copyrightstatement{TEXT} %% This section is optional and can be used for copyright transfers.


\introduction  %% \introduction[modified heading if necessary]

% INTEREST OF CLOUDS IN GENERAL
Accurate and continuous monitoring of cloud properties contributes to a profound understanding of atmospheric processes and their subsequent impacts on various Earth systems \citep{liou1992radiation}. It provides essential insights for weather predictions and climate dynamics \citep{hu2004application,petzold2015global}.
% WAYS OF CLOUD OBSERVATION
Many diverse instruments are dedicated towards cloud detection. Observation methods can be divided into two primary distinct categories: downward satellite-based observations \citep{roy2017satellite, martin2008satellite} and upward ground-based observations with all-sky cameras, lidar, radar and other instruments \citep{wilczak1996ground}. The principal aim of satellite-based observations is to investigate the upper regions of clouds, facilitating the examination and analysis of global atmospheric patterns and climate conditions over expansive geographical areas \citep{schiffer1983international, boers2006satellite, geer2017growing, varnai2018satellite}. In contrast, ground-based cloud observation excels in the surveillance of localized regions, furnishing valuable data pertaining to the lower segments of clouds by giving information on cloud altitude, cloud extent, and cloud typology \citep{bower2000ace, zhou2019cloud}. Combination of these two measurement techniques enhances our overall comprehension of cloud behavior \citep{mokhov1994analysis, schreiner1993comparison, yamashita2012ground, yoshimura2013contribution}.

% MORE ABOUT GROUND BASED OBSERVATIONS
Ground-based observations have been extensively used in recent years and have become a viable means to detect, study and identify cloud formations \citep{paczynski2000monitoring, skidmore2008using, tzoumanikas2016effect, ugolnikov2017noctilucent, Mommert2020, tzoumanikas2016effect, roman2022retrieval}. As technological evolution has ushered in a new era of monitoring methodologies \citep{mandat2014all}, the utilization of infrared thermal cameras has emerged as a promising avenue for atmospheric investigations through precise radiometric measurements \citep{Szejwach1982, Shaw_2013, liandrat2017cloud, lopez2017contribution, Klebe2014, nikolenko2021infrared}.

Because of their practical use, high sensitivity, low-cost, operating range and wide field-of-view (FOV) \citep{Rogalski2011,Rogalski2014,Kimata2018}, it makes them particularly useful for medicine \citep{ring2012infrared}, agriculture \citep{ishimwe2014applications}, aerial \citep{wilczak1996ground}, defense \citep{gallo1993low, akula2011thermal}, surveillance \citep{wong2009effective}, weather forecast \citep{sun2008whole, Liandrat2017}, or even astronomical related applications to determine the cloud cover fraction during operations and therefore assess the quality of scientific observations \citep{Klebe2010, lewis2010radiometric,Klebe2012,Klebe2014, reil2014update}. Indeed, uncooled infrared microbolometers array sensors working in the 10-12 $\mu m$ spectral band can directly detect the LWIR emission of both clouds and the atmospheric background, excluding the scattered light of the sun or starlight \citep{Houghton1972}. These LWIR sensors are able to provide high-contrast images and allow fine radiometric measurements to detect low-emissivity cirrus clouds \citep{lewis2010radiometric,Shaw_2013}.

Across recent years, multiple automatic ground-based observations systems have been developed. For example, the infrared cloud imager (ICI) \citep{ICI}, can detect clouds and assess cloud coverage both in daylight and at nighttime with a dedicated infrared sensor. \citet{Sharma2015} designed an instrument to detect of the cloud infrared radiations to be used in search for a potential site for India’s National Large Optical Telescope project. The development of the Radiometric All-Sky Infrared Camera (RASICAM, referenced in \citealt{lewis2010radiometric} and \citealt{reil2014update}) was aimed at enabling automated, real-time quantitative evaluation of nighttime sky conditions for the Dark Energy Survey (DES see \citealt{DES}). This specialized camera is designed for the precise detection of the location, movement, and optical properties of thin, high-altitude cirrus clouds and contrails by measuring their brigthness temperature against the sky background. The all-sky infrared visible analyzer (ASIVA, see \citealt{Klebe2014}) is a similar instrument whose primary goal is to provide radiometrically calibrated imagery in the LWIR band to estimate fractional sky cover and sky/cloud brightness temperature, emissivity and cloud height. The ASC-200 system \citep{rs13091852} combines information from two all-sky cameras facing the sky operating in both the visible spectrum (450-650 nm) and the LWIR band.

% RELATION TO ASTRONOMY
As next-generation cosmological surveys require more demanding precision on photometric observations (implying better characterization of the atmosphere), monitoring telescope instruments field-of-view (FOV) with LWIR thermal cameras may provide significant asset to ; (i) classify observations quality in real-time; (ii) evaluate potential cloud coverage \citep{Smith2008, liandrat2017cloud, amt-11-5549-2018, rs13091852}; (iii) estimate precipitable water vapor content (PWV) \citep{amt-15-1563-2022, Hack2023, environsciproc2023026033}.
%Multiple ground-based all-sky and narrow field-of-view (FOV) infrared instruments have demonstrated reliable measurements of sky radiance \citep{Klebe2012, Klebe2014}.
%For the upcoming Vera Rubin Observatory cosmological survey, preliminary work has demonstrated the feasability to monitor the entire sky with uncooled infrared cameras. 

In this study, we plan to address the first objective. We use a LWIR thermal infrared camera with a specifically chosen narrower FOV that aims to image the surrounding area of the StarDICE telescope FOV. The StarDICE metrology experiment \citep{stardice1} that aims at measuring CALSPEC \citep{Bohlin2014} spectrophotometric standard stars absolute flux at the 0.1\% relative uncertainty level. Enhanced characterization of atmospheric conditions are required to reach the target sensitivity. As a preliminary step, basic knowledge of the atmosphere conditions in the telescope FOV may provide valuable insights onto the quality of spectrophotometric measurements. However, these kind of infrared instruments operate at high framerate and produce considerable amounts of data which makes it extremely difficult to analyze by human observers. Therefore, to determine cloud presence in infrared images, deep convolutional neural networks appear to be a viable approach to process images in real-time. Multiple models relying on convolutional neural network (CNN) have been developed such as: CloudSegnet \citep{dev2019cloudsegnet}, CloudU-Net \citep{CloudUNet} CloudU-Netv2 \citep{CloudUNetv2}, SegCloud \citep{SegCloud}, TransCloudSeg \citep{TransCloudSeg}, CloudDeepLabV3 \citep{CloudDeepLabV3}, ACLNet \citep{makwana2022aclnet}, DeepCloud \citep{DeepCloud}, CloudRaednet \citep{shi2022cloudraednet}, DMNet \citep{DMNet} and DPNet \cite{DPNet}. Nonetheless, these methodologies exclusively address RGB-colored images \citep{HYTA, dev2016colorbased}. Colors or hue provides the essential of information for segmentation (especially red and blue channels). In the case of LWIR thermal images, we implement a model capable of achieving comparable accuracy for single-channel gray-scaled images. Inspired by their large successes in image classification and structure detection for various computer vision tasks, we propose a new dedicated deep-learning framework. Our approach is specifically designed towards gray-scaled infrared images and consists of: (i) classify images (e.g, detect if any cloud is present onto the image and discriminates between clear and cloudy images); (ii) identify cloud structure (e.g., generate a pixel-based probabilistic segmentation map and verify if the CCD camera FOV is impacted).
%Others are specifically dedicated toward satellite imagery \cite{kim2018deep, kanu2020cloudx, chen2023novel}.
The main contributions of this paper are threefold:

% \begin{itemize}
%     \item IRIS-CloudDeep: new infrared sky image classification and cloud segmentation using deep-learning framework for ground-based radiometric camera observation;
%     \item Comparison of the proposed solution to various state-of-the-art methods;
%     \item Viable method for vetting astronomical observations quality in real-time.
% \end{itemize}

The remainder of the paper is structured as follows. Background about the scientific context and related works are presented in Section \ref{sec:background}. Section \ref{sec:setup} details the experimental setup and dataset. Section \ref{sec:framework} introduces the proposed framework, describing deep-learning architectures and training procedures. Experimental results and comparisons with other methods are provided in Section \ref{sec:experiments}. Relevant matters and future perspectives are discussed in Section \ref{sec:discussion}. Section \ref{sec:conclusion} depicts a summary and finally concludes the paper.

\section{Background}
\label{sec:background}

\subsection{Motivation}

\textcolor{orange}{KS : may need to shorten the motivation subsection to be more synthetic and straight to the point. Is it necessary to explain the StarDICE project principles ? Maybe not}

StarDICE represents one of the initiatives focused on creating a measurement process that bridges the gap between laboratory flux standards (such as silicon photodiodes calibrated by NIST) and the stars found in the CALSPEC library of spectrophotometric references \citep{Bohlin2020}. Since type 1a supernovae and most astronomical  surveys rely on the calibration of these standard stars for their measurements \citep{Bohlin2011, Betoule2013, Rubin2015, Scolnic2015, Currie2020, Brout2022, rubin2022uniform}, successfully establishing this connection with high precision effectively addresses the calibration challenge associated with the Hubble diagram. The StarDICE proposal encompasses a five-step sequence, as illustrated in Figure \ref{fig:schematics_stardice}. This process hinges on the near-field calibration of a dim but stable light source, emitting less than 1 $\mu$W of optical power. It serves as a distant ($\sim$ 100 m) in-situ reference for a compact astronomical telescope.
One of the largest remaining source of systematic uncertainty is Earth atmosphere transmission \citep{stubbs2012addressing, Stubbs2015, Li2016}. It is dependent on many environmental conditions and processes, including: absorption and scattering by molecular constituents ($\text{O}_{2}$, $\text{O}_{3}$, and others), absorption by precipitable water vapor (PWV), scattering by aerosols, and shadowing by larger ice crystals and water droplets in clouds that is independent of wavelength and responsible for \textit{gray extinction} \citep{Burke2010, Burke2017}. Current atmospheric transmission or extinction models do not integrate the possible impact of cloud. Indeed, formation of thin clouds through the condensation of water droplets and ice can result in clouds that are extremely faint and cannot be perceived in the visible spectrum with the naked eye. These clouds often exhibit complex spatial structures, as demonstrated in \citealt{Burke2014}. The condensation process takes place along well-defined boundaries in temperature and pressure, which are influenced by PWV. This phenomenon commonly categorizes observing conditions as either \textit{photometric} or \textit{non-photometric}.
To address this issue, we propose to use an infrared thermal camera providing high-sensitivity radiometric measurements of the sky radiance in the atmosphere transparency window (10-12 \textmu m). Previous work demonstrated our ability to calibrate it with better than 0.1 $W/m^{2}/sr$ on each pixel, corresponding to the added radiance of a theoretical cirrus cloud at surface temperature of 205 K with a visible optical depth of $\tau$ = 0.02. With the help of cloud spatial structure, this instrument may be the key to assess photometric observations quality and label science images with superior than state-of-the-art accuracy.

\begin{figure}[t]
	\includegraphics[width=\hsize]{figures/schematics_stardice.png}
	\caption{Schematics of the metrology chain of the StarDICE experiment. Each arrow represents a step in the chain and the label gives the order of magnitude of the beam intensity. The steps in upper left box are conducted at NIST \citep{Larason2008} and result in a silicon photodiode calibrated against an electrical substitution cryogenic radiometer. The StarDICE collaboration builds the dedicated bench transfer system in the right box, designed to reach $10^{-19} \: W/cm2/nm$ flux sensitivity. Primary calibration stars catalog is then used by large cosmological surveys for absolute flux calibration.}
    \label{fig:schematics_stardice}
\end{figure}

\subsection{Related work}

In recent years, numerous cloud sky/cloud segmentation algorithms have been introduced along with the increased development of all-sky ground-based cloud monitoring stations \citep{Long2006, Yang2012, rs12111902, ASC2, Mommert2020, amt-15-3629-2022}.
Indeed, cloud segmentation is big challenge for remote sensing applications as clouds come in various shapes and forms. Therefore, the most modern common approach aims to use computer vision encoder-decoder architectures algorithms and train them onto very specific publicly available cloud image databases such as: SWIMSEG \citep{dev2016colorbased}, SWINSEG \citep{SWINSEG, dev2017nighttime}, SWINySEG \citep{dev2019cloudsegnet}, WSISEG \citep{SegCloud}, HYTA \citep{HYTA} and TLCDD. Most proposed solutions are focused on visible RGB images. CloudSegNet \citep{dev2019cloudsegnet} is a light-weight deep-learning encoder/decoder network that detects clouds onto daytime and nighttime visible color images. CloudU-Net \citep{CloudUNet} modifies CloudSegNet architecture by adding dilated convolution, skip connection, and fully connected conditional random field (CRF, see \citealt{McCallumCRF}) layers to demonstrates better segmentation performance overall. It uses the powerful U-Net architecture \citep{UNET} originally applied to medical image segmentation. CloudU-Netv2 \citep{CloudUNetv2} replaces the upsampling in CloudU-Net with bilinear upsampling, improves discrimination ability of features representation and uses rectified Adam optimizer (rADAM is a variant of the Adam stochastic optimizer \citep{ADAM} that introduces a term to rectify the variance of the adaptive learning rate, see \citealt{RADAM}). SegCloud \citep{SegCloud} has been trained onto 400 images and possesses a symmetric encoder-decoder structure and outputs low/high-level cloud feature maps to the same resolution of input images. TransCloudSeg \citep{TransCloudSeg} adresses the loss of global information due to limited receptive field size of the filters in CNN by proposing an hybrid model containing both the CNN and a transformer \citep{TRANSFORMER} as the encoders to obtain different features. CloudDeepLabV3+ \citep{CloudDeepLabV3} designs a lightweight ground-based cloud image adaptive segmentation method  that integrates multi-scale fea­tures aggregation and multi-level attention feature enhancement. ACLNet \citep{makwana2022aclnet} uses EfficientNet-B0 as the backbone, “à trous spatial pyramid pooling” (ASPP see \citealt{ATROUS}) to learn at multiple receptive fields, and global attention module (GAM see \citealt{GAM}) to extract fine-grained details from the image. It provides lower error rate, higher recall and higher F1-score than state-of-art cloud segmentation models. DeepCloud \citep{DeepCloud} uses the method of Fisher vector encoding which is applied to executing the spatial feature aggregation and high-dimensional feature mapping on the raw deep convolutional features. CloudRaednet \citep{shi2022cloudraednet} proposes a residual attention-based encoder-decoder network and train it over the SWINySEG dataset. \citealt{MACNN} introduces a novel deep model named multiscale attention convolutional neural network (MACNN) to obtain different receptive fields by using different hole rates for the filters and propose the attention module to learn the attention coefficients in order to reflect different importance of pixels. DMNet \citep{DMNet} proposes a novel cloud detection network that aims to achieve information complementarity by exploiting the different properties of the features at different levels of the encoder, so as to strengthen the detailed information of high-level features and make the low-level features have more semantics. DPNet \citep{DPNet} possesses an encoder-decoder structure with Dual Pyramid Pooling Module (DPPM). They process the feature maps of different scales in the encoder through a technique known as dual pyramid pooling. They also implement the Encoder-Decoder Constraint (EDC) to relieve information loss in the process of encoding and decoding.

Majority of these models are typically structured using an encoder-decoder architecture, which is the primary innovation brought forth by incorporating CNNs \citep{oshea2015introduction}. The encoder is tailored to acquire representational features, facilitating the extraction of semantic information while the decoder reconstructs these representational features into the segmentation mask, allowing for pixel-level classification \citep{badrinarayanan2017segnet, Alzubaidi2021ReviewOD}.

Others have proposed solutions for all-sky infrared image classification.
\citet{Liu2021} applies pre-processing steps (smoothing noise reduction, enhancement through top-hat transformation and a high-pass filtering, edges detection) before extracting features that are useful for distinguishing cirriform, cumuliform, and waveform clouds. A simple rectangle method as supervised classifier is applied. They find a 90\% agreement between a priori classificatoin carried out manually by visual inspection and their algorithm on 277 images. \citet{SUN2011278} suggested: (i) a method for determining clear sky radiance threshold; (ii) cloud identification combined threshold method with texture method; (iii) an algorithm to retrieve cloud base height from downwelling infrared radiance. They showed that structural features are better than texture features in classifying cloud. \citet{amt-11-5351-2018} proposed a three-step process: (i) pre-processing; (ii) feature extraction; (iii) classification method to group images into five cloud categories (stratiform, cumuliform, waveform, cirriform and clear) based on manifold and texture features using support vector machine (SVM see \citealt{cortes1995support}). Their experimental results demonstrate the higher recognition rate with an increase of 2\%-10\% on ground-based infrared images datasets. Nevertheless, these methods class clouds into separate categories based on their typology. Until now, all the previously examined approaches, while effective within their specific domains, proved to be unsuccessful when applied to our particular use case. Therefore, we propose a new deep-learning framework based on CNNs and U-Net architectures to identify cloud images and detect cloud structure in real-time.

\section{Experimental setup and datasets}
\label{sec:setup}

\subsection{Description of the infrared thermal camera}

Our instrument is an infrared thermal camera, specifically the FLIR Tau2, which operates in the long-wave infrared (LWIR) band, covering the 8-14 $\mu m$ range. It features a focal plane array (FPA) consisting of 640 $\times$ 512 uncooled microbolometers, capturing images at a framerate of 9 Hz. To achieve a narrow field of view (FOV), we've paired the camera with a 60 mm F1.25 lens. The primary purpose of deploying this instrument on the equatorial mount adjacent to the StarDICE photometric telescope is to continuously assess the atmospheric conditions (specifically gray extinction) within the line of sight of the visible CCD camera during observations.
Through meticulous calibration, radiative transfer calculations, and data analysis using simulations, we can extract valuable information about the sky to monitor real-time atmospheric conditions. In Figure \ref{fig:infrared_system}, we show the instrument mounted on the equatorial mount inside the observatory dome, complete with the necessary command and control equipment. We also monitor the surrounding and internal temperatures of the camera in real-time to correct for temperature-related variations in sensor response.
The device is controlled and commanded via the ThermalCapture ThermalGrabber USB 2.0 interface\footnote{\url{https://thermalcapture.com/}}, which grants access to full 14-bit radiometric raw data. We have developed an open-access \textsc{Python} program, available on GitHub\footnote{\url{https://github.com/Kelian98/tau2_thermalcapture}}, to control the camera's functions and capture images. These images are saved in \textsc{FITS} format. To correct for anisotropies caused by fixed pattern noise (FPN), we employ an enhanced flat-field calibration source at regular intervals of approximately 30 seconds. True scene radiances are computed from raw images with per-pixel coefficient matrices from pre-established calibration procedure.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.9\hsize}
        \includegraphics[width=\textwidth]{figures/infrared_instrument.pdf}
        %\caption{Subfigure 1}
        %\label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\hsize}
        \includegraphics[width=\textwidth]{figures/sample_sky_image.pdf}
        %\caption{Subfigure 2}
        %\label{fig:subfig2}
    \end{subfigure}
    \caption{\textit{Top:} infrared instrument installed onto the equatorial table of the StarDICE experiment at Observatoire de Haute-Provence. \textit{Bottom:} sample of raw infrared thermal image in ADU.}
    \label{fig:infrared_system}
\end{figure}

\subsection{Datasets and pre-processing}

A substantial quantity of images is essential for the effective training and testing of both the classifier and segmentation algorithms. Our dataset comprises LWIR sky images that we captured ourselves. It encompasses XXX sky images acquired by the infrared thermal camera. To speed up computations and minimize memory consumption, we downsampled the original-sized images (640 $\times$ 512) into a 160 $\times$ 128 resolution by binning them in a 4$\times$4 format. Cloudy sky images were collected during a three-night period at Observatoire de Haute-Provence in January 2023 (43° 55' 51" N, 5° 42' 48" E). Conversely, cloud-free images were obtained over a short time span during the same month.

To compensate for the lack of cloud-free images and prevent potential biases in training due to data imbalance, we generated synthetic cloud-free images to create a composite dataset containing as many images as the cloudy dataset. These synthetic images replicate realistic observations by simulating 2D horizontal gradients, mimicking the increase in sky downwelling radiance as the camera's field of view tilts toward high zenith angles (i.e., low elevation angles). Realistic sources of noises affecting uncooled infrared thermal cameras are introduced, including: read noise, fixed pattern noise, sky noise and narcissus effect. This addition ensures that the spatial noise in the synthetic images closely resembles that of actual cloud-free images. Figure \ref{fig:synthetic_clear_sky_image} illustrates a typical cloud-free image alongside a synthetically generated one, with spatial noise indicated for each. It's worth noting that the absolute analog-to-digital unit (ADU) value has no impact, as the data is normalized prior to training

\begin{figure}[t]
	\includegraphics[width=\hsize]{figures/synthetic_clear_sky_image.pdf}
	\caption{Comparison between real observed clear sky image (top) and synthetically generated realisitic image (bottom). Spatial noise is marked on the top right corner of each image.}
    \label{fig:synthetic_clear_sky_image}
\end{figure}

All images and masks are visually inspected. Samples presenting artifacts such as tree branches from surroundings or buildings in the FOV corners are discarded. As the camera acquisition framerate enables to record up to 9 images per second, the pre-processing algorithm included constraints on consecutive image selection based on their timeseries. Selected frames are taken from at least 2 seconds between each other to introduce a wider range of displayed clouds.

Ground-truth masks identifying cloud structure on cloud images were manually created through multiple steps of stretching procedures using \textsc{astropy} \citep{Astropy2013,Astropy2018} methods for each image in the dataset. They consist of boolean 2D array of the same image size, where \textit{True} identified pixels represent cloud pixels and \textit{False} identified pixels represent clear sky areas. This step could not be automated as cloud optical depths, brightness temperatures and structure differ significantly. Indeed, the segmentation model aims at automating this tedious time consuming operation in real-time during observations. Figure \ref{fig:cloud_images_ground_truth} depicts two raw images with their associated manually generated ground-truth cloud masks for training purposes.

\begin{figure}[t]
	\includegraphics[width=\hsize]{figures/cloud_images_ground_truth.pdf}
	\caption{Representative infrared cloudy sky images and their corresponding manually created ground-truth masks. The masks are binary images, where zero represents clear sky and one represents cloud. \textcolor{orange}{KS : the last two rows are too similar images. Ideal would be to have the last one in the middle row and add a very cloudy image.}}
    \label{fig:cloud_images_ground_truth}
\end{figure}

Furthermore, we performed multiple random augmentations (e.g, flip, shear, rotate, shift and zoom) on each original image to artificially enlarge the size of each dataset and reduce overfitting \citep{perez2017effectiveness, 8388338, yang2022image}.
All augmented images are produced through the random sequential applications of these five distinct operations to initial images.
These operations are executed with a random varying degree of intensity contained in specific ranges. Random rotations are applied within an amplitude ranging from -45 to +45 degrees. Shear is introduced with a random magnitude ranging from -0.2 to +0.2. Shifting operations are carried out with a maximum ratio of 10\% in both width and height directions to avoid generation of unrealistic symmetric structures. Zoom operation is applied within the range of 1 to 1.8. No other transformation such as histogram equalization or contrast enhancment is applied to prevent any bias or alteration in the segmentation performance. After the selection and augmentation procedures, we conducted a visual examination of all the created sky/cloud images to ensure that they appear realistic. Since all the parameters in the image augmentation process undergo controlled adjustments, our generated images closely mirror authentic sky/cloud scenes.
Datasets are for both models are splitted into training, testing and validation subsets with ratio of in the ratio of 70\%, 20\% and 10\% respectively. Table \ref{tab:dataset} depicts the numbers and properties of the datasets. 

\begin{table}[t]
\begin{center}
    \caption{Overview of the collected image datasets for classification and segmentation.}
    \begin{tabular}{c c c c} 
        \tophline
     Image type & \# training & \# validation & \# total \\ [1.0ex]
     \hline
     Clear & XXXX & XXXX & XXXX \\ [1.0ex]
     \hline
     Cloud & XXXX & XXXX & XXXX \\ [1.0ex]
     \hline
    \end{tabular}
    \belowtable{}
    \label{tab:dataset}
    \end{center}
    

\end{table}

\section{Methodology}
\label{sec:framework}

\subsection{Overall framework of IRIS-CloudDeep}

In this section, we outline the architectural designs of two distinct deep-learning models tailored for cloud-related tasks. On the one hand, we implement a classifier for cloud classification using Convolutional Neural Networks (CNN) \citep{lecun1995convolutional, Krizhevsky2012}, whose specific goal is to discriminate between cloud-free and cloudy images. On the other hand, the segmentation for cloud structure detection is performed via an optimized U-Net model \citep{UNET}. The output probability map can later be thresholded according to the user needs in order to produce the desired predicted binary segmentation map. Figure \ref{fig:architecture_schematic} illustrates the proposed deep-learning framework compared to conventional segmentation algorithms.

\begin{figure*}[t]
	\includegraphics[width=\hsize]{figures/architecture_schematic-crop.pdf}
	\caption{Schematic diagram of common-used deep-learning architectures (top) and this paper solution (bottom). The input image is a 160 × 128 radiometric grayscale image of the sky from the LWIR instrument. Classifier model output is a boolean.}
    \label{fig:architecture_schematic}
\end{figure*}


\subsection{Image classification}

For our image classification model, we employed a Convolutional Neural Network (CNN) architecture \citep{SHARMA2018377} derived from the VGG-16 network \citep{simonyan2015deep}, which has proven to be highly effective in image recognition tasks \citep{canziani2016analysis} without introducing much complexity as ResNets \citep{ResNet}. VGG-16 is a convolutional neural network that is 16 layers deep whereas ResNet generally have higher depths. This modified network was designed with the primary objective of distinguishing between images that contain clouds and those that do not and is similar to SegCloud \citep{SegCloud}.

The VGG-16 architecture serves as the backbone of our model. It consists of a series of convolutional layers, followed by rectified linear unit (ReLU) activation \citep{agarap2018deep} and max-pooling operations. This configuration facilitates the extraction of hierarchical features that are important for accurate classification. We retained the convolutional layers and their weight parameters from the original VGG-16 model to benefit from the network's ability to capture intricate visual patterns.

To adapt the network to our binary classification task, we made adjustments to the fully connected layers towards the end of the architecture. Specifically, we replaced the original fully connected layers with a custom set of fully connected layers. These modified layers were designed to map the learned features to the two classes of interest: images containing clouds and images without clouds. The final output layer consisted of two neurons, each representing one of the classes, and a softmax activation function was applied to obtain the class probabilities.

Furthermore, we incorporated dropout layers after the fully connected layers to mitigate overfitting and enhance the generalization capability of our model. This architectural modification helped us strike a balance between model complexity and performance, ensuring that the network could effectively differentiate between cloud and non-cloud images.

The model is trained on a comprehensive dataset encompassing both cloud and cloud-free infrared images, with corresponding ground truth labels.
Figure \ref{fig:schematics_classification_model} depicts the schematic diagram of the architecture.
 %The categorical cross-entropy loss function is employed for optimization, aiming to minimize the discrepancy between predicted and actual classifications.

\begin{figure*}[t]
	\includegraphics[width=\hsize]{figures/schematics_segmentation_model.png}
	\caption{\textcolor{orange}{Update this plot for the actual classifier architecture. Keep one of the following caption written here and rearrange it so that it matches the diagram.}
		Schematic diagram of the classifier model architecture. Boxes represent cross-sections of square feature maps.  The networks contain an encoder network, a corresponding decoder network, and a final Softmax classifier. Each map's dimensions are indicated on its lower left, and its number of channels are indicated above it. Half-grey boxes represent maps for which half of their channels are copied. The input image is a 160 × 128 radiometrically calibrated grayscale image of the sky from the LWIR instrument. Classifier model output is a boolean. Arrows represent operations, specified by the legend-notably, blue arrows represent convolutions, while gray ones represent copying (skip connections). Tensor dimensions at the output of each block are specified.
        VGG-8 architecture for CIFAR-10/SVHN: We implement a non-standard VGG-8 architecture (without Batch Normalization), which is similar to the standard VGG-11 [40] network but with five convolution layers instead of eight. Each convolution layer has 3 × 3 filters with padding 1 and stride 1. The convolution layer is followed by ReLU non-linearity and a max pooling layer with 2 × 2 filter, no padding and stride 2 resulting in a downsampling factor of 2. The convolution layers are followed by 3 fully connected layers of dimensions 128, 128 and 10. We use ReLU non-linearity after each fc layer except the final one.
        CNN architecture for CIFAR-10/SVHN: The network consists of three convolution layers with 3$\times$3 filters, 0 padding and stride 1. The convolution layers are followed by a ReLU non-linearity. We use max pooling in this work with a filter size of 2$\times$2, no padding and stride 2 which results in a downsampling of the features by a factor of 2. The three convolution layers have 6, 16 and 32 filters respectively. Finally, a Global Average Pooling (GAP) is applied and a fully connected (fc) outputs logits over the number of classes.}
        \label{fig:schematics_classification_model}
\end{figure*}

\subsubsection{Encoder block}

Our modified VGG-16-derived encoder comprises a stack of 10 convolutional layers and 5 max-pooling layers, each meticulously designed to facilitate feature extraction. Within each convolutional layer, a sequence of operations unfolds, encompassing convolution, batch normalization \citep{ioffe2015batch, bjorck2018understanding}, and rectified linear unit (ReLU) activation \citep{agarap2018deep}.

The initial step in this process is the convolution operation, where input feature maps are convolved with trainable filters featuring a 3$\times$3 window size and a stride of 1. Subsequently, batch normalization is employed to normalize the obtained feature maps. This normalization step is pivotal in accelerating the convergence of our model during training and counteracting issues related to the vanishing gradient problem.

Following batch normalization, ReLU activation is applied, introducing non-linearity into the network and expanding its capacity for feature representation. Notably, the early convolutional layers focus on capturing fine-grained visual details, such as shapes and edges, while the deeper convolutional layers leverage these foundational features to compute higher-level and more complex semantic characteristics, aligning with the principles elucidated by Liang et al. in 2017.

Additionally, our encoder network integrates 5 max-pooling layers, thoughtfully positioned after the convolutional layers. These layers play a pivotal role in enhancing the network's translation invariance, a critical characteristic for robust image classification. Each max-pooling layer performs subsampling on the input feature maps using a 2$\times$2 window size and a stride of 2, effectively reducing the size of output feature maps by half while capturing salient features. The incorporation of max-pooling layers further fortifies the network's ability to discern crucial information, ultimately contributing to the accuracy of our cloud image classification task.

\subsubsection{Decoder block}

The decoder network in our CNN classifier model, adapted from a modified VGG-16 architecture, plays a pivotal role in the restoration of high-level feature maps to the original image resolution, enabling precise cloud image classification.

Comprising 5 upsampling layers and 10 convolutional layers, the decoder network progressively increases the spatial resolution of feature maps while enhancing segmentation accuracy. Four of the upsampling layers utilize pooling indices from corresponding max-pooling layers in the encoder network, optimizing feature restoration with minimal computational overhead. However, this approach may slightly compromise cloud boundary details.

Recognizing the importance of preserving boundary information, the final upsampling layer employs a distinct strategy. It directly utilizes feature maps duplicated from the first max-pooling layers of the encoder network to improve cloud boundary recognition. This process involves bilinear interpolation, doubling the size of feature maps, and concatenating them with duplicated feature maps, ultimately achieving high-resolution cloud feature maps.

In summary, our decoder network systematically restores high-level cloud feature maps to the original image resolution through a series of upsampling layers. This process is vital for preserving details and enabling accurate cloud image classification within our modified VGG-16-based CNN classifier.

\subsubsection{Binary classifier}

The binary classifier is located after the decoder network to achieve final image classification. It is specifically designed for tasks where there are only two possible classes or categories, often labeled as $1$ and $0$, in this case "cloud" or "clear", contrary to the softmax classifier which applies to multiclass problems. The classification process is realized through a sigmoid activation function. The output is a single-channel probability image, where each pixel's value is interpreted as the probability of it belonging to the cloud class. In practical terms, the pixel-wise predictions are obtained by applying a threshold to the probabilities. Pixels with probabilities exceeding a predefined threshold are classified as belonging to the cloud class, while those below the threshold are classified as non-cloud.


\subsection{Image segmentation}

For cloud structure identification, the U-Net architecture is adopted owing to its effifiency in semantic segmentation tasks. The U-Net model comprises an encoder and a decoder, facilitating the capturing of context-rich features and precise delineation of cloud structures. The encoder integrates convolutional and max-pooling layers to progressively downsample the input image, thereby capturing high-level features. These features are then decoded using up-convolutions and skip connections, enabling the reconstruction of the segmented cloud structures.  Figure \ref{fig:sample_segmentation} illustrates some examples of infrared cloud images and their corresponding ground-truth masks and predictions. Figure \ref{fig:schematics_segmentation_model} shows the architecture of the segmentation model.

\begin{figure*}[t]
    \centering
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/test_dl_15epochs.png}
        %\label{fig:sub1}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/test_dl_15epochs2.png}
        %\label{fig:sub2}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/test_dl_15epochs3.png}
        %\label{fig:sub3}
    \end{subfigure}

    \caption{Sample of input images (left), associated ground-truth binary masks (center) and probabilistic map predictions (right) of IRIS-CloudDeep segmentation model. \textcolor{orange}{Use other images to not be redundant with precent figures + only titles for the first row : cloud image \& ground-truth binary mask \& model prediction. Change the colormaps so that the left and right columns are not made from the same colors.}}
    \label{fig:sample_segmentation}
\end{figure*}

\begin{figure*}[t]
	\includegraphics[width=\hsize]{figures/schematics_segmentation_model.png}
	\caption{\textcolor{orange}{Update this plot for the actual segmentation architecture. Keep one of the following caption written here and rearrange it so that it matches the diagram.}
		Schematic diagram of U-Net based segmentation model architecture. Boxes represent cross-sections of square feature maps. The U-net model contains two parts: down-sampling (left half) and up-sampling (right half). After each convolutional layer, ReLU activation function (improves the computational speed of the training stage )and BN function were applied to effectively capture non-linearities in data and speedup the training
        Each map's dimensions are indicated on its lower left, and its number of channels are indicated above it. Half-grey boxes represent maps for which half of their channels are copied. The input image is a 160 × 128 radiometrically calibrated grayscale image of the sky from the LWIR instrument. The output image is a probabilistic mask prediction of pixels being cloudy or clear. Arrows represent operations, specified by the legend-notably, blue arrows represent convolutions, while gray ones represent copying (skip connections). Tensor dimensions at the output of each block are specified. Classifier model output is a boolean.}
    \label{fig:schematics_segmentation_model}
\end{figure*}

\subsubsection{Encoder block}

The encoder block of IRIS-CloudDeep segmentation model consists of four convolution layers and four max-pooling layers. We feed a normalized and binned 4x4 radiometrically calibrated image of a fixed input size (160 × 128 pixels) into the model.
The initial convolutional layer applies a set of learnable filters to the input image, extracting low-level features. Additional convolutional layers increase the complexity of learned features by applying convolutions to the feature map generated by the previous layer, creating a hierarchy of increasingly abstract features. After a set of convolutional layers, a max-pooling layer is applied to downsample the feature map. Max-pooling helps to reduce the spatial dimensions of the feature map while retaining the most salient information.

\subsubsection{Decoder block}

The decoder takes the high-level features generated by the encoder and aims to restore the spatial resolution of the original input image. In the U-Net architecture, the decoder is connected to the encoder via skip connections, enabling the network to combine local and contextual information. The decoder starts with an upsampling operation to increase the spatial dimensions of the feature map. A skip connection connects the upsampled feature map from the decoder with the corresponding feature map from the encoder. This enables the network to leverage both local and global context information. Following the concatenation, a series of convolutional layers are applied. These layers refine the combined feature map, gradually transitioning from abstract features to more detailed information. The final convolutional layer produces the segmentation mask of the U-Net model.

\subsubsection{Model output}

The output the classifier is a boolean representing whether the image contains cloud or not. On the contrary, the image segmentation model output is a probability mask that assigns a probability value to individual pixels, indicating their potential belonging in the cloud category.Subsequently, a straightforward thresholding technique is applied to transform the probability mask into a binary map. The threshold for labeling is obtained from the Receiver Operating Curve (ROC) analysis specific to our experimental conditions.

\subsection{Training procedure and implementation details}

%\subsubsection{Loss function}
The CNN classifier is trained on the dataset containing both cloudy and synthetic clear sky images. The U-Net model is trained on cloudy images only which is the labeled dataset containing infrared images with pixel-wise cloud structure annotations (ground-truth masks). The loss function employed for training is the binary cross-entropy, which quantifies the difference between predicted probabilities and actual binary class labels for each instance in the dataset. Mathematically, given an instance's true binary label $y$ (0 or 1) and the predicted probability $p$ of it belonging to class 1, the binary cross-entropy loss $\mathcal{L}$ is calculated as:
\begin{equation}
	\mathcal{L} = -\frac{1}{N}\sum_i y_i\cdot\log\left(f_w(x_i)\right) + (1-y_i)\cdot\log\left(1-f_w(x_i)\right)
\end{equation}
where $\mathcal{L}$ is the binary cross-entropy loss. $N$ is the total number of instances in the dataset, $i$ index represents an individual instance, $y_{i}$ is the $i$-th true binary label (0 or 1) and $f_w(x_i)$ is the predicted probability that belongs to class 1, based on the model with parameters $w$. The goal of training is to minimize this loss function by adjusting the model parameters weights $w$ to better align the predicted probabilities $f_w(x_i)$ with the true labels $y_{i}$.

Both architectures are implemented in \textsc{Python} with the \textsc{keras} \citep{Keras} subpackage of \textsc{TensorFlow} framework \citep{TensorFlow} and \textsc{flax} \citep{Flax} neural network library of \textsc{jax} \citep{Jax}. IRIS-CloudDeep is trained on the GPU cluster infrastructure of the MESO@LR\footnote{\url{https://meso-lr.umontpellier.fr/}} high-performance computing center with an NVIDIA Quadro RTX 6000. Images are normalized and downsampled into a fixed 160 $\times$ 128 resolution format to speed up computations and to capture the global trend. Models are trained over 50 and 100 epochs respectively using the ADAM optimizer \citep{ADAM}. We set the batch size to 16 images with a dynamic learning rate initial value of $\lambda$ = 0.001 that decreases through the training epochs following the relation : $\lambda(\text{epoch}) = 0.001 \cdot 0.5^{\left\lfloor \frac{\text{epoch}}{10} \right\rfloor}
$. We also set an earlystopping watchdogs that interrupts the training if the loss value doesn't decrease below a certain threshold after 15 epochs. Hyperparameters are fine tuned with the \textsc{optuna} framework \citep{Optuna} using the sampling strategy algorithm.

\section{Experiments}
\label{sec:experiments}

\subsection{Performance metrics}

\textcolor{orange}{KS : question to WK : do you see any other relevant performance metrics to add ?}

In order to evaluate the performance of the proposed models, we adopt six metrics: precision (P), accuracy (A), recall (R), F1-score (F1), intersection over union (IoU) and error rate (ER) between the ground-truths and the predictions. For the segmentation model, accuracy and interesection over union are averaged over the pixels to get the mean pixel accuracy (mA) and the mean intersection over union (mIoU). All of these parameters are defined in the following equations,
\begin{equation}
    P = \frac{TP}{TP + FP}
\end{equation}
\begin{equation}
    A = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}
\begin{equation}
    R = \frac{TP}{TP + FN}
\end{equation}
\begin{equation}
    F1 = \frac{2 \cdot P \cdot R}{P + R}
\end{equation}
\begin{equation}
    IoU = \frac{TP}{TP + FP + FN}
\end{equation}
\begin{equation}
    ER = \frac{FP + FN}{TP + TN + FP + FN}
\end{equation}
with True Positives (TP) the number of correctly classified positive instances; False Positives (FP) the number of negative instances that were incorrectly classified as positive; False Negatives (FN) the number of positive instances that were incorrectly classified as negative; True Negatives (TN) the number of correctly classified negative instances.

\subsection{Results}

\textcolor{orange}{KS: the text in red is copy-pasted from relevant articles to give some ideas for the writting of the associated sections. I took what I found to be the best written of all. We should present the results in a similar manner. }

\textcolor{red}{Table 1 reports that SegCloud achieves a high average ac-
curacy of 96.24\%, which further objectively proves its effec-
tiveness. Moreover, SegCloud performs well on whole-sky
images with different cloud cover conditions and achieves
96.98\% accuracy on clear-sky images, 95.2\% accuracy on
partial-cloud images, and 99.44\% near-perfect accuracy on overcast-sky images. These experimental results show that
the SegCloud is effective and accurate for cloud segmenta-
tion and can provide a reference for future cloud segmenta-
tion research.}

\begin{table*}[t]
\begin{center}
    \caption{Evaluation metrics for the proposed IRIS-CloudDeep framework. (P = precision, mPA = mean pixel accuracy, R = recall, F1 = F1-score, mIoU= mean intersection over union, ER = error-rate)}
    \begin{tabular}{c c c c c c c} 
    \tophline \hline
     Model & P [\%] & mPA [\%] & R & F1 [\%] & mIoU [\%] & ER [\%] \\ [1.0ex]
     \hline
     Classifier & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
     Segmentation & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\
     \hline
    \end{tabular}
    \belowtable{}
    \end{center}
\end{table*}



\subsubsection{Classifier}

\textcolor{red}{The performance of the ResNet model is somewhat sensitive to the learning rate and momentum, but outcomes are very similar for learning rates of the order of (1-3)\% and momentum
values 0.7-0.9. However, we do find significant variations
between independent training runs despite the use of manual
random seeding, which we attribute to random scheduling
during the GPU acceleration and the relatively small training
sample size for this type of model. In the following, we report
on the results of the best of five independent training runs.
Figure 2 shows that we find validation sample accuracies of
the order of $\sim$ 80\%, peaking around 87\% for individual training epochs. After $\sim$ 20 epochs, the training sample loss becomes mostly stationary, meaning that the model does not improve.The test sample loss, however, is subject to significant
variations, which we attribute to the relatively small sample
size. Training of the ResNet model leads to rather high
validation sample accuracies of the order of 85\% after only
$\sim$10 training epochs. We adopt this accuracy and number of
epochs in our further analysis. We find f1-scores of the order of 0.88. The f1 score is defined as the harmonic mean of precision and recall and serves as a measure for the overall performance of a binary classifier, where 1 denotes a flawless classification and lower values denote flawed classification results. Training our ResNet adaptation for 100 epochs takes 6.9 hr, 10 epochs of training takes accordingly 41 minutes.}
\subsubsection{Segmentation}

\textcolor{red}{We adopt the following set of hyperparameters for our lightGBM model: a maximum depth of each tree of 5, 500 estimators, a learning rate of 0.25, 30 leaves per tree, 100 examples required to form a leaf, a = 10, and b = 100. This configuration leads to a training sample accuracy of 96\% and a
test sample accuracy of 95\%. The accuracy on the validation
sample, which was neither used in the training of the model nor
in the tuning of the hyperparameters, is 95\% too. The f1 score
on the validation sample is 0.94, underlining the good
performance of the trained model.
The training of the entire training sample using the selected
hyperparameters and a five-fold cross-validation takes 12 s on
a standard desktop computer. Figure 3 shows the feature importances extracted from the
final trained model. The feature importance used here is defined
as the number of times a feature is used in this model
throughout all individual decision trees. The comparison shows
that environmental parameters that affect sky brightness are
extremely important, followed by the subregion location.
Actual subregion properties and their time differentials follow,
the latter of which only have a small-but not negligible impact on the model results.}

\subsection{Framework effectiveness/performance}

\subsubsection{Losses trend}

\textcolor{red}{The trend of binary cross-entropy loss for training and testing sets are shown in Fig. 2. We observe that the loss saturates after
a few thousand iterations, and the model exhibits comparable
loss performance for both training and testing sets. We choose
the CloudSegNet model with the lowest validation loss for our
subsequent experiments.}

\begin{figure}[t]
	\includegraphics[width=\hsize]{figures/loss_trend.png}
	\caption{\textcolor{orange}{Keep the caption but transform the plot following it : Training and testing losses of IRIS-CloudDeep models over
    epochs. The dashed curve is for training whereas the continuous curve is for testing. Blue and red curves denote the classifier and segmentation models respectively.}}
    \label{fig:loss_trend}
\end{figure}

\subsubsection{Receiver Operating Curve (ROC)}

\textcolor{red}{As discussed above, the output of CloudSegNet is a prob-ability mask, wherein each pixel indicates the degree of belongingness to the cloud category. Since the ground-truth maps are binary in nature, it is necessary to convert the probabilistic output into binary maps as well. We employ a Receiver Operating Curve (ROC) technique to understand the impact of the threshold on the performance. We vary the threshold from 0 to 1 in steps of 0.01, and record the False Positive Rate (FPR) and True Positive Rate (TPR) of cloud detection. Figure 3 shows the resulting ROC curve. The area under the ROC curve (AUC) is 0.97, indicating the competitive performance of CloudSegNet. The ROC curve provides an opportunity to choose a threshold, based on the trade-off between FPR and TPR. In our experiments, we choose the threshold of 0.5 to convert the probabilistic map into a binary sky/cloud image, which is very close to equal true and false positive rates. Of course, this threshold can be further adjusted by the user, based on the specific requirements for TPR and/or FPR. Figure 4 shows some sample outputs of our proposed approach. Visual inspection of additional images from our SWINySeg dataset confirms that CloudSegNet can successfully identify cloud pixels from nychthemeron images.}


\begin{figure}[t]
	\includegraphics[width=\hsize]{figures/roc_curve.png}
	\caption{ROC curve comparison of different segmentation models. The AUC value represents the area under the curve. \textcolor{orange}{Update this plot for the classifier and segmentation algorithms. Use one color of choice for each. Be consistent with colors from the loss plot above.}}
    \label{fig:roc_curve}
\end{figure}

\subsubsection{Confusion matrices}

\textcolor{red}{The cloud detection probability for a single subregion is
$\sim$85\% using ResNet and $\sim$95\% using lightGBM. Since clouds
typically cover more than one subregion, the probability that
any subregion in a set of N subregions that actually include
clouds increases exponentially with N. In the same way, the
probability to miss clouds decreases. For example, the
probability to miss the detection of clouds in three different
subregions with the lightGBM classifier is $\sim$0.053 = 10-4.
Hence, the confidence in detecting the presence of clouds
anywhere on the sky is much higher than the probability to
detect them in a single subregion, supporting the usefulness of
this machine-learning approach.
We further investigate the performance of our models using
a confusion matrix, which not only provides information on the
overall classification accuracy, but also additional information
on the rate of false-positive and false-negative classifications.
Here, a false-positive classification means a subregion that has
been predicted to contain clouds, although this is not the case.
A false-negative classification refers to a subregion that
contains clouds that are not identified by the classifier. The
confusion matrices for both methods used here are shown in
Figure 4.
As Figure 4 shows, the false-negative and false-positive rates
using the lightGBM classifier are rather small at 5.2\% and
2.7\%, respectively. We point out that the false-negative rate is
roughly a factor 2 higher than the false-positive rate, which
might be slightly affected by the class imbalance inherent to the
training data sample (see Section 2.2), but is mostly likely due
to the classifier’s inability to identify non-opaque clouds that
were labeled in the training data set. This effect, as well as
additional shortcomings potentially related to the insufficient
size of the training sample (see Section 5.2.1), is much more
obvious in the results of the ResNet classifier, which achieves a
false-negative rate of 29\% and a false-positive rate of 3.6\%,
underlining the insufficient performance of this classifier. The
comparison of these numbers support the suitability of the
lightGBM approach for this task, which is able to identify
clouds with high confidence.}

\begin{figure}[t]
	\includegraphics[width=\hsize]{figures/confusion_matrices.png}
	\caption{ Confusion matrices for the methods considered in the framework. The normalized confusion matrix demonstrates the performance of the proposed model in classifying tiles containing obscurant clouds. The model tends to miss-classify tiles with tiles near the edge of the obscurant clouds since the human labels considered the edges clear although containing very light occlusion.\textcolor{orange}{Again update this plot. Also keep the same color fashion as the 2 previous plot for the classifier and segmentation models to not be confusing. Maybe red and blue will do the trick.}}
    \label{fig:confusion_matrices}
\end{figure}

We use this augmented composite datasets of 10,000 images respectively to train the classifier and identifer. Random sampling is applied on each composite dataset, and standard train-test-validation split is perormed . Figure \ref{} depicts trends for the binary cross-entropy losses for training and testing subsets with various hyperparameters. \textsc{Optuna} framework allows ease of hyperparameters optimization. We observe that the loss saturates after a few hundred iterations for all attempts, and the model exhibits comparable loss performance for both training and testing sets. We select the model and layer parameters with the lowest validation loss for our subsequent experiments.



\section{Discussion}
\label{sec:discussion}

\subsection{Limited benchmarking comparison}

Comparing machine-learning models optimized for different types of input data can be meaningful in certain contexts, but it requires careful considerations. The most critical factor is the nature of the data. As state-of-the art models presented in Section \ref{sec:background} are optimized for different type of data (3-channels RGB images) and it may seem uncoherent to compare the performance with single-channel grayscale infrared images. Indeed, these data types have distinct characteristics, and models may perform differently based on these differences.
Some models \cite{SUN2011278, Liu2021, amt-11-5351-2018} have been proposed to target infrared images with categorization tasks. Consider whether the models can be adapted or fine-tuned to work with both RGB and infrared data is challenging. This might involve multi-modal learning approaches \citep{liu2018multimodal, li2020deep, MMST} or transfer learning techniques \citep{manzo2021voting, Wang21, zhou2021novel} which are not the intended purpose of this work.

Nevertheless, we attempt to evaluate the robustness of our segmentation model by testing its ability to generalize to other datasets including: SWIMSEG \citep{dev2016colorbased}, SWINSEG \citep{SWINSEG, dev2017nighttime}, SWINySEG \citep{dev2019cloudsegnet}, WSISEG \citep{SegCloud}, HYTA \citep{HYTA} and TLCDD. Images containing elements other than clouds or sky are cropped to prevent the model to learn unwanted features and avoid subsequent misinterpretations. We transform RGB images into gray-scaled images with \textsc{OpenCV} \citep{OpenCV} color conversion method \texttt{COLOR\textunderscore RGB2GRAY} defined by the following equation,
\begin{equation}
    \text{RGB} \rightarrow \: \text{Gray} = 0.299 \cdot \text{R} + 0.587 \cdot \text{G} + 0.114 \cdot \text{B}
    \label{eq:rgb_to_gray}
\end{equation}
where R, G and B are respectively red, green and blue channels of the input color image.
Metrics for each dataset are summarized in Table \ref{tab:datasets_comparison}.

Although results show that most cloud structures in the images can be
accurately recognized by models, we find this method of transforming color images to grayscale not optimal. Due to Rayleigh scattering producing strong blue color channels \citep{bates1984rayleigh}, our framework is less efficient on the transformed publicly available datasets. Additional work may be needed to improve conversion of RGB color to grayscale images with similar contrast to infrared thermal images. Nevertheless, we confirm its ability to generalize on unseen data.

\begin{table*}[t]
    \begin{center}
        \caption{Evaluation metrics for the proposed segmentation model on publicly available state-of-the-art datasets. Note that RGB color images are transformed into gray-scale images as IRIS-CloudDeep segmentation model is optimized for this type of data. Best values are denoted in bold font. (P = precision, mPA = mean pixel accuracy, R = recall, F1 = F1-score, mIoU= mean intersection over union, ER = error-rate). \textcolor{orange}{May need to get rid of WSISEG as images are all-sky and not cropped...}}
        \begin{tabular}{c c c c c c c} 
        \tophline \hline
         Dataset & P [\%] & mPA [\%] & R & F1 [\%] & mIoU [\%] & ER [\%] \\ [1.0ex]
         \hline
         SWIMSEG$^{*}$ & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
         SWINSEG$^{*}$ & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
         SWINySEG$^{*}$ & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
         WSISEG$^{*}$ & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
         HYTA$^{*}$ & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
         TLCDD$^{*}$ & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\ [1.0ex]
         \textbf{LWIRISEG} & XXXX & XXXX & XXXX & XXXX & XXXX & XXXX \\
         \hline
        \end{tabular}
        \label{tab:datasets_comparison}
        \belowtable{$^{*}$RGB-color images are transformed into gray-scale images according to Equation \ref{eq:rgb_to_gray}.}
        \end{center}
    \end{table*}


\subsection{Impact of training data quality and quantity}

We investigate the impact of the training subset quality and quantity.
It is often challenging to produce non-ambiguous ground-truth masks as it is subject to biases with manual work. As clouds can take on various forms in images, establishing a precise definition of cloud presence and differentiation from sky background is difficult. Machine learning model's susceptibility to cirrus clouds and those that are not completely opaque is greatly influenced by the quality of the training data. Contrary to \cite{Mommert2020}, we do not expect clouds to be entirely opaque. Indeed, as we image in LWIR band, the sensor is not susceptible to traditional visible spectrum phenomena such as air glow. This consideration is apparent in the false-positive (FP) rates of classifiers and segmentation models which sometimes indicates cloud presence where the expected airmass gradient is located.

To evaluate the effect of the training set size, we train the exact same classifier and segmentation architectures while selecting different number of images. Results for different subset sizes are listed in Table \ref{}.

Furthermore, small number of nights leads to less variety in cloud shapes and structures.

\textcolor{red}{
In the case of our ResNet approach, a steady rise of both
accuracy and F1 score can be observed through all training
sample sizes used in this analysis. The fact that neither metric
plateaus indicates that the training sample size required to max
out the performance of the ResNet model has not been reached
and that this model will benefit from additional training data.
We fit a power-law function of the form f (x; a, b) to the ResNet accuracy values in Table 1
and find a saturation accuracy of only 92\%. Furthermore, we
find through extrapolation that our ResNet approach requires of
the order of 20,000 training samples to achieve an accuracy of
90\%. We acknowledge that this extrapolation may not be highly accurate, but it certainly provides reasonable estimates
of the orders of magnitudes for both the maximum accuracy
that can be expected and the training sample size. Based on
these estimates, we conclude that our ResNet approach in this
form is extremely expensive compared to our lightGBM model.
We find lightGBM performances that are comparable to
those reported in Section 4.2 for training sample sizes of the
order of 1000 image examples. Even in the case of only 100
image examples, an accuracy above 90\% can be reached. This
result implies that the lightGBM approach is useful even if only
a small training sample is available. We furthermore conclude
that more then 1000 training examples will not significantly
improve the performance of this model.}


\subsection{Comparison of libraries: \textsc{flax} vs \textsc{keras}}

When comparing training deep learning models with \textsc{keras} and \textsc{flax}, we observed notable distinctions in terms of speed and data loading capacity. Keras, being a high-level framework, offers a straightforward and user-friendly experience, but its training speed may be limited, particularly with larger datasets and batch sizes. In contrast, \textsc{flax}, with its low-level nature and support for hardware acceleration, demonstrated impressive speed improvements, especially when dealing with substantial datasets or larger batch sizes. The ability of \textsc{flax} to efficiently leverage hardware resources, coupled with its flexibility in managing data loading, enables smoother training on extensive datasets and better overall training. These factors make \textsc{flax} an appealing choice for tasks demanding scalability and computational efficiency. Table \ref{} depicts speed comparison and model size for the same segmentation model architecture. Results show a speed increase of XXX\% and smaller model disk size being XXX\% smaller than the \textsc{keras} trained model.


\begin{table}[t]
    \begin{center}
        \caption{Comparison of model size and computation speed for the same segmentation model implemented with \textsc{keras} and \textsc{flax}.}
        \begin{tabular}{c c c} 
        \tophline \hline
         Library & Run-time [s] & Size [MB]\\
         \hline
         Keras & XXXX & 24 \\ [1.0ex]
         Flax & XXXX & 14 \\ [1.0ex]
         \hline
        \end{tabular}
        \belowtable{}
        \end{center}
    \end{table}
    

\subsection{Future perspectives}

The framework established in this paper is planned to be used in real-time with dedicated computing module. Functions of this
module will include the following aspects: (i) classifying infrared sky images obtained by the imaging system in real-time, (ii) analyzing cloud-labeled sky images and deriving the corresponding cloud structure and cover using the segmentation algorithm; (iii) generating alerts/flag in accordance with the results; (iv) fully-autonomous processing.

Training on a larger dataset may be an additional step towards improving the accuracy and robustness of our framework. As next-year will bring large amount of data thanks to the telescope system remote operations capabilities, we have the potential to capture a broader range of sky atmospheric conditions, making our models more adept at handling real-world variations. However, it's crucial to maintain a balance between the model's complexity and the available computational resources, as larger datasets can lead to increased training times and resource demands. Additionally, staying vigilant for shifts in data distribution over time is essential to ensure that our models remain adaptive and responsive to evolving experimental conditions.

\conclusions[Conclusion]%% \conclusions[modified heading if necessary]
\label{sec:conclusion}

In this paper, we proposed IRIS-CloudDeep, a deep-learning framework for classification and segmentation of LWIR
ground-based thermal images. As far as we know, it is the framework that attempts to apply two sequential models for complementary tasks on single-channel gray-scaled infrared images. Specifically, we presented the CNN-based classifier and the U-Net based segmentation model tailored to extract cloud structures on pre-identified cloud images. Extensive experimental results on a combination of self-acquired data and transformed publicly available datasets have demonstrated
the effectiveness and performances of the proposed framework.

\textcolor{orange}{KS : Remind some key metrics of the classifier and segmentation algorithms}

We successfully increased the size of training, testing and validation subsets with random application of augmentation methods. We developed an accurate simulation tool to produce realistic clear sky images. In the future, additional data will be collected by the infrared instrument, capturing various weather conditions. The framework may be re-trained on heavier datasets that will probably increase its accuracy.  Furthermore, if enough data is collected with many different cloud categories and proven-to-be accurate radiometric calibration, we will be able to expand the segmentation model to perform cloud typology through multi-label segmentation. Finally, the current trained model is expected to process data in real-time for the StarDICE experiment in order to: (i) give live alerts to remote observers in the case of cloud detection; (ii) set a flag in CCD images for quality sorting in post-processing.

%% The following commands are for the statements about the availability of data sets and/or software code corresponding to the manuscript.
%% It is strongly recommended to make use of these sections in case data sets and/or software code have been part of your research the article is based on.





%\codeavailability{TEXT} %% use this section when having only software code available

%\dataavailability{} %% use this section when having only data sets available

\codedataavailability{The source code is freely available online at \url{https://github.com/kelian98/IRIS-CloudDeep}. The datasets and other supporting materials will be made available from corresponding author Kélian Sommer (kelian.sommer@umontpellier.fr) upon request.} %% use this section when having data sets and software code available

\authorcontribution{KS conceived the instrument, collected data, pre-processed the dataset, created ground truths masks for segmentation and realistic synthetic data for classification. WK, RB and KS designed the framework. WK and RB performed the experiments. KS wrote the paper and collected relevant literature. WK and RB revised the manuscript. AB, JCT and BP proposed constructive suggestions on the revision of the article. } %% this section is mandatory

\competinginterests{The authors declare that they have no conflict of interest.} %% this section is mandatory even if you declare that no competing interests are present

%\disclaimer{TEXT} %% optional section

\begin{acknowledgements}
This work has been realized with the support of MESO@LR-Platform at the University of Montpellier. Some of the results in this paper have been derived using \textsc{astropy} \citep{Astropy2013, Astropy2018}, \textsc{flax} \citep{Flax}, \textsc{jax} \citep{Jax}, \textsc{keras} \citep{Keras}, \textsc{matplotlib} \citep{Matplotlib}, \textsc{numpy} \citep{Numpy}, \textsc{opencv-python} \citep{opencv-python}, \textsc{pandas} \citep{Pandas}, \textsc{scipy} \citep{scipy} and \textsc{tensorflow} \citep{TensorFlow}. \textcolor{orange}{Add thanks for the people that revised the manuscript : JCT and AB; on the same model as \citet{stardice1} acknowledgements}
\end{acknowledgements}


%sampleavailability{TEXT} %% use this section when having geoscientific samples available

%\videosupplement{TEXT} %% use this section when having video supplements available


%\appendix
%\section{}    %% Appendix A

%\subsection{}     %% Appendix A1, A2, etc.


%\noappendix       %% use this to mark the end of the appendix section. Otherwise the figures might be numbered incorrectly (e.g. 10 instead of 1).

%% Regarding figures and tables in appendices, the following two options are possible depending on your general handling of figures and tables in the manuscript environment:

%% Option 1: If you sorted all figures and tables into the sections of the text, please also sort the appendix figures and appendix tables into the respective appendix sections.
%% They will be correctly named automatically.

%% Option 2: If you put all figures after the reference list, please insert appendix tables and figures after the normal tables and figures.
%% To rename them correctly to A1, A2, etc., please add the following commands in front of them:

\appendixfigures  %% needs to be added in front of appendix figures

\appendixtables   %% needs to be added in front of appendix tables

%% Please add \clearpage between each table and/or figure. Further guidelines on figures and tables can be found below.








%% REFERENCES

%% The reference list is compiled as follows:

%\begin{thebibliography}{}

%\bibitem[AUTHOR(YEAR)]{LABEL1}
%REFERENCE 1

%\bibitem[AUTHOR(YEAR)]{LABEL2}
%REFERENCE 2

%\end{thebibliography}

%% Since the Copernicus LaTeX package includes the BibTeX style file copernicus.bst,
%% authors experienced with BibTeX only have to include the following two lines:
%%

%\bibliographystyle{abbrv}
\bibliography{biblio.bib}
%%
%% URLs and DOIs can be entered in your BibTeX file as:
%%
%% URL = {http://www.xyz.org/~jones/idx_g.htm}
%% DOI = {10.5194/xyz}


%% LITERATURE CITATIONS
%%
%% command                        & example result
%% \citet{jones90}|               & Jones et al. (1990)
%% \citep{jones90}|               & (Jones et al., 1990)
%% \citep{jones90,jones93}|       & (Jones et al., 1990, 1993)
%% \citep[p.~32]{jones90}|        & (Jones et al., 1990, p.~32)
%% \citep[e.g.,][]{jones90}|      & (e.g., Jones et al., 1990)
%% \citep[e.g.,][p.~32]{jones90}| & (e.g., Jones et al., 1990, p.~32)
%% \citeauthor{jones90}|          & Jones et al.
%% \citeyear{jones90}|            & 1990



%% FIGURES

%% When figures and tables are placed at the end of the MS (article in one-column style), please add \clearpage
%% between bibliography and first table and/or figure as well as between each table and/or figure.

% The figure files should be labelled correctly with Arabic numerals (e.g. fig01.jpg, fig02.png).


%% ONE-COLUMN FIGURES

%%f
%\begin{figure}[t]
%\includegraphics[width=8.3cm]{FILE NAME}
%\caption{TEXT}
%\end{figure}
%
%%% TWO-COLUMN FIGURES
%
%%f
%\begin{figure*}[t]
%\includegraphics[width=12cm]{FILE NAME}
%\caption{TEXT}
%\end{figure*}
%
%
%%% TABLES
%%%
%%% The different columns must be seperated with a & command and should
%%% end with \\ to identify the column brake.
%
%%% ONE-COLUMN TABLE
%
%%t
%\begin{table}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table}
%
%%% TWO-COLUMN TABLE
%
%%t
%\begin{table*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table*}
%
%%% LANDSCAPE TABLE
%
%%t
%\begin{sidewaystable*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{sidewaystable*}
%
%
%%% MATHEMATICAL EXPRESSIONS
%
%%% All papers typeset by Copernicus Publications follow the math typesetting regulations
%%% given by the IUPAC Green Book (IUPAC: Quantities, Units and Symbols in Physical Chemistry,
%%% 2nd Edn., Blackwell Science, available at: http://old.iupac.org/publications/books/gbook/green_book_2ed.pdf, 1993).
%%%
%%% Physical quantities/variables are typeset in italic font (t for time, T for Temperature)
%%% Indices which are not defined are typeset in italic font (x, y, z, a, b, c)
%%% Items/objects which are defined are typeset in roman font (Car A, Car B)
%%% Descriptions/specifications which are defined by itself are typeset in roman font (abs, rel, ref, tot, net, ice)
%%% Abbreviations from 2 letters are typeset in roman font (RH, LAI)
%%% Vectors are identified in bold italic font using \vec{x}
%%% Matrices are identified in bold roman font
%%% Multiplication signs are typeset using the LaTeX commands \times (for vector products, grids, and exponential notations) or \cdot
%%% The character * should not be applied as mutliplication sign
%
%
%%% EQUATIONS
%
%%% Single-row equation
%
%\begin{equation}
%
%\end{equation}
%
%%% Multiline equation
%
%\begin{align}
%& 3 + 5 = 8\\
%& 3 + 5 = 8\\
%& 3 + 5 = 8
%\end{align}
%
%
%%% MATRICES
%
%\begin{matrix}
%x & y & z\\
%x & y & z\\
%x & y & z\\
%\end{matrix}
%
%
%%% ALGORITHM
%
%\begin{algorithm}
%\caption{...}
%\label{a1}
%\begin{algorithmic}
%...
%\end{algorithmic}
%\end{algorithm}
%
%
%%% CHEMICAL FORMULAS AND REACTIONS
%
%%% For formulas embedded in the text, please use \chem{}
%
%%% The reaction environment creates labels including the letter R, i.e. (R1), (R2), etc.
%
%\begin{reaction}
%%% \rightarrow should be used for normal (one-way) chemical reactions
%%% \rightleftharpoons should be used for equilibria
%%% \leftrightarrow should be used for resonance structures
%\end{reaction}
%
%
%%% PHYSICAL UNITS
%%%
%%% Please use \unit{} and apply the exponential notation


\end{document}
